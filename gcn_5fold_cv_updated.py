# -*- coding: utf-8 -*-
"""gcn_5fold_cv

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XHoEGTV_1Crt3xgYP2GyciKvmjabLssC
"""

## comment it for supercomputer
# !pip install funcsigs

from __future__ import (absolute_import, division, print_function, unicode_literals)

# from .layers import *
# from .models import *
# from .utils import *

import math
import torch
from torch.nn.parameter import Parameter
from torch.nn.modules.module import Module
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import scipy.sparse as sp
import time
import argparse
import torch.optim as optim
import random
import pandas as pd
import matplotlib.pyplot as plt
import re
import os
from collections import defaultdict
from funcsigs import signature
from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, auc, confusion_matrix, classification_report

"""# **args**"""

class MyArgs():
  ## hyperparameters
  data_split_seed = 42
  cos_sim_thresh = 0.99
  epochs = 200
  lr = 0.01
  weight_decay = 5e-4
  hidden = 16
  dropout = 0.5
  output_dir = '/N/u/snouri/Carbonate/thesis/gcn_dis_hyper_search/cv_shuffle_on/'

  ## do not change these parameters
  data_dir = '/N/u/snouri/Carbonate/thesis/gcn_dis_hyper_search/'
  seed = 42
  readmission_mode = 'discharge'
  feature_mode = 'pool'
  shuffle_index_list = True
  save_features = False
  no_cuda = False
  fastmode = False

args = MyArgs()

"""# **layers.py**"""

class GraphConvolution(Module):
    """
    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907
    """

    def __init__(self, in_features, out_features, bias=True):
        super(GraphConvolution, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.weight = Parameter(torch.FloatTensor(in_features, out_features))
        # print("I AM HERE!")
        if bias:
            self.bias = Parameter(torch.FloatTensor(out_features))
        else:
            self.register_parameter('bias', None)
        self.reset_parameters()

    def reset_parameters(self):
        stdv = 1. / math.sqrt(self.weight.size(1))
        self.weight.data.uniform_(-stdv, stdv)
        if self.bias is not None:
            self.bias.data.uniform_(-stdv, stdv)

    def forward(self, input, adj):
        support = torch.mm(input, self.weight)
        output = torch.spmm(adj, support)
        if self.bias is not None:
            return output + self.bias
        else:
            return output

    def __repr__(self):
        return self.__class__.__name__ + ' (' \
               + str(self.in_features) + ' -> ' \
               + str(self.out_features) + ')'

"""# **models.py**"""

# from pygcn.layers import GraphConvolution

class GCN(nn.Module):
    def __init__(self, nfeat, nhid, nclass, dropout):
        super(GCN, self).__init__()

        self.gc1 = GraphConvolution(nfeat, nhid)
        self.gc2 = GraphConvolution(nhid, nclass)
        self.dropout = dropout

    def forward(self, x, adj):
        x = F.relu(self.gc1(x, adj))
        x = F.dropout(x, self.dropout, training=self.training)
        x = self.gc2(x, adj)
        out = F.log_softmax(x, dim=1)
        # print(out)
        return out

"""# **utils.py**"""

def encode_onehot(labels):
    classes = set(labels)
    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in
                    enumerate(classes)}
    labels_onehot = np.array(list(map(classes_dict.get, labels)),
                             dtype=np.int32)
    return labels_onehot

def load_data(path="/content/", dataset="cora"):
    """Load citation network dataset (cora only for now)"""
    print('Loading {} dataset...'.format(dataset))

    idx_features_labels = np.genfromtxt("{}{}.content".format(path, dataset),
                                        dtype=np.dtype(str))
    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)
    labels = encode_onehot(idx_features_labels[:, -1])

    # build graph
    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)
    idx_map = {j: i for i, j in enumerate(idx)}
    edges_unordered = np.genfromtxt("{}{}.cites".format(path, dataset),
                                    dtype=np.int32)
    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),
                     dtype=np.int32).reshape(edges_unordered.shape)
    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),
                        shape=(labels.shape[0], labels.shape[0]),
                        dtype=np.float32)

    # build symmetric adjacency matrix
    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)

    features = normalize(features)
    adj = normalize(adj + sp.eye(adj.shape[0]))

    idx_train = range(140)
    idx_val = range(200, 500)
    idx_test = range(500, 1500)

    features = torch.FloatTensor(np.array(features.todense()))
    labels = torch.LongTensor(np.where(labels)[1])
    adj = sparse_mx_to_torch_sparse_tensor(adj)

    idx_train = torch.LongTensor(idx_train)
    idx_val = torch.LongTensor(idx_val)
    idx_test = torch.LongTensor(idx_test)

    return adj, features, labels, idx_train, idx_val, idx_test
    
def normalize(mx):
    """Row-normalize sparse matrix"""
    rowsum = np.array(mx.sum(1))
    r_inv = np.power(rowsum, -1).flatten()
    r_inv[np.isinf(r_inv)] = 0.
    r_mat_inv = sp.diags(r_inv)
    mx = r_mat_inv.dot(mx)
    return mx


def accuracy(output, labels):
    preds = output.max(1)[1].type_as(labels)
    correct = preds.eq(labels).double()
    correct = correct.sum()
    return correct / len(labels)


def sparse_mx_to_torch_sparse_tensor(sparse_mx):
    """Convert a scipy sparse matrix to a torch sparse tensor."""
    sparse_mx = sparse_mx.tocoo().astype(np.float32)
    indices = torch.from_numpy(
        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))
    values = torch.from_numpy(sparse_mx.data)
    shape = torch.Size(sparse_mx.shape)
    return torch.sparse.FloatTensor(indices, values, shape)

"""# **train.py**"""

args = MyArgs()

# from pygcn.utils import load_data, accuracy
# from pygcn.models import GCN

# Training settings
# parser = argparse.ArgumentParser()
# parser.add_argument('--no-cuda', action='store_true', default=False,
#                     help='Disables CUDA training.')
# parser.add_argument('--fastmode', action='store_true', default=False,
#                     help='Validate during training pass.')
# parser.add_argument('--seed', type=int, default=42, help='Random seed.')
# parser.add_argument('--epochs', type=int, default=200,
#                     help='Number of epochs to train.')
# parser.add_argument('--lr', type=float, default=0.01,
#                     help='Initial learning rate.')
# parser.add_argument('--weight_decay', type=float, default=5e-4,
#                     help='Weight decay (L2 loss on parameters).')
# parser.add_argument('--hidden', type=int, default=16,
#                     help='Number of hidden units.')
# parser.add_argument('--dropout', type=float, default=0.5,
#                     help='Dropout rate (1 - keep probability).')

# args = parser.parse_args()
args.cuda = not args.no_cuda and torch.cuda.is_available()
print("args.cuda:", args.cuda)

np.random.seed(args.seed)
torch.manual_seed(args.seed)
if args.cuda:
    torch.cuda.manual_seed(args.seed)

# Load data
# adj, features, labels, idx_train, idx_val, idx_test = load_data()

"""# **load data**"""

train_df = pd.read_pickle(args.data_dir + 'train_pretrain_pooled_outputs')
num_train = len(train_df)
train_ones = np.sum(train_df["labels"])
val_df = pd.read_pickle(args.data_dir + 'val_pretrain_pooled_outputs')
num_val = len(val_df)
val_ones = np.sum(val_df["labels"])
test_df = pd.read_pickle(args.data_dir + 'test_pretrain_pooled_outputs')
num_test = len(test_df)
test_ones = np.sum(test_df["labels"])

print("Number of train examples:", num_train)
print("Number of val examples:", num_val)
print("Number of test examples:", num_test)
print(".....................")
print("Number of train 1's:", train_ones)
print("Number of val 1's:", val_ones)
print("Number of test 1's:", test_ones)
print(".....................")


data_list = [train_df, val_df, test_df]

data = pd.concat(data_list)

data.reset_index(drop=True, inplace=True)

num_ids = len(data)
print("Number of nodes=", num_ids) 

map_ids_to_ints = {}
map_ints_to_ids = {}
all_mean_embeddings = []
for i in range(num_ids):
  map_ids_to_ints[data['ids'][i]] = i
  map_ints_to_ids[i] = data['ids'][i]
  mean_embed = torch.from_numpy(np.mean(data['pooled outputs'][i], axis=0))
  all_mean_embeddings.append(mean_embed)

flat_embeddings = [item.item() for sublist in all_mean_embeddings for item in sublist]

features = np.array(flat_embeddings).reshape(num_ids, -1)

cos = nn.CosineSimilarity(dim=0, eps=1e-8)
adj_dict = {}
edges_list = []
for i in range(num_ids):
  adj_dict.setdefault(i, list())
  for j in range(i+1, num_ids):
    adj_dict.setdefault(j, list())
    sim = cos(all_mean_embeddings[i], all_mean_embeddings[j])
    if sim>args.cos_sim_thresh:
      adj_dict[i].append(j)
      adj_dict[j].append(i)
      edge = []
      edge.append(i)
      edge.append(j)
      edges_list.append(edge)  

assert(len(adj_dict) == num_ids)   
print("Number of edges=", len(edges_list)) 

flat_edges = [item for sublist in edges_list for item in sublist]
edges = np.array(flat_edges).reshape(-1, 2)

## train/val/test index
# idx_train = range(num_train)
# idx_val = range(num_train, num_train + num_val)
# idx_test = range(num_train + num_val, num_train + num_val + num_test)

index_list = list(range(num_train + num_val + num_test))
labels = list(data['labels'])
ad_ids = list(data['ids'])
train_val_test = zip(index_list, labels, ad_ids)

zero_list = []
one_list = []
for i in train_val_test:
  if i[1]==0:
    zero_list.append(i)
  else:
    one_list.append(i)   

if args.shuffle_index_list:
  random.Random(args.data_split_seed).shuffle(zero_list)     
  random.Random(args.data_split_seed).shuffle(one_list)   

train_zeros = num_train - train_ones
val_zeros = num_val - val_ones
test_zeros = num_test - test_ones

# train_index_ones = one_list[:train_ones]
# val_index_ones = one_list[train_ones:train_ones+val_ones]
# test_index_ones = one_list[train_ones+val_ones:]


# train_index_zeros = zero_list[:train_zeros]
# val_index_zeros = zero_list[train_zeros:train_zeros+val_zeros]
# test_index_zeros = zero_list[train_zeros+val_zeros:]

# train_index_1 = [i[0] for i in train_index_ones]
# train_index_0 = [i[0] for i in train_index_zeros]
# idx_train = train_index_1 + train_index_0

# val_index_1 = [i[0] for i in val_index_ones]
# val_index_0 = [i[0] for i in val_index_zeros]
# idx_val = val_index_1 + val_index_0

# test_index_1 = [i[0] for i in test_index_ones]
# test_index_0 = [i[0] for i in test_index_zeros]
# idx_test = test_index_1 + test_index_0


#if args.shuffle_index_list:
# random.Random(args.data_split_seed).shuffle(index_list)
#idx_train = index_list[:num_train]
#idx_val = index_list[num_train : num_train + num_val]
#idx_test = index_list[num_train + num_val : num_train + num_val + num_test]


adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),
                    shape=(len(labels), len(labels)),
                    dtype=np.float32)


# build symmetric adjacency matrix
adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)

features = sp.csr_matrix(features, dtype=np.float32)

features = normalize(features)
adj = normalize(adj + sp.eye(adj.shape[0]))

features = torch.FloatTensor(np.array(features.todense()))
labels = torch.LongTensor(np.array(labels))
adj = sparse_mx_to_torch_sparse_tensor(adj)
print("Num of 1's:", torch.sum(labels).item())

# random.Random(args.data_split_seed).shuffle(idx_train) 
# random.Random(args.data_split_seed).shuffle(idx_val) 
# random.Random(args.data_split_seed).shuffle(idx_test)

# #print(torch.sum(labels[idx_train])
# train_ad_ids = [ad_ids[i] for i in idx_train]
# val_ad_ids = [ad_ids[i] for i in idx_val]
# test_ad_ids = [ad_ids[i] for i in idx_test]

# def common_data(list1, list2): 
#     result = False
  
#     # traverse in the 1st list 
#     for x in list1: 
  
#         # traverse in the 2nd list 
#         for y in list2: 
    
#             # if one common 
#             if x == y: 
#                 result = True
#                 return result  
                  
#     return result 
      
# print("common data between train and val:", common_data(train_ad_ids, val_ad_ids)) 
# print("common data between train and test:", common_data(test_ad_ids, train_ad_ids)) 

# idx_train = torch.LongTensor(idx_train)
# idx_val = torch.LongTensor(idx_val)
# idx_test = torch.LongTensor(idx_test)

def train(epoch):
    # output_list = []
    t = time.time()
    model.train()
    optimizer.zero_grad()
    output = model(features, adj)
    # output_list.append(output)
    loss_train = F.nll_loss(output[idx_train], labels[idx_train])
    acc_train = accuracy(output[idx_train], labels[idx_train])
    loss_train.backward()
    optimizer.step()

    if not args.fastmode:
        # Evaluate validation set performance separately,
        # deactivates dropout during validation run.
        model.eval()
        output = model(features, adj)

    loss_val = F.nll_loss(output[idx_val], labels[idx_val])
    acc_val = accuracy(output[idx_val], labels[idx_val])
    print('Epoch: {:04d}'.format(epoch+1),
          'loss_train: {:.4f}'.format(loss_train.item()),
          'acc_train: {:.4f}'.format(acc_train.item()),
          'loss_val: {:.4f}'.format(loss_val.item()),
          'acc_val: {:.4f}'.format(acc_val.item()),
          'time: {:.4f}s'.format(time.time() - t))


def test(fold):
    model.eval()
    output = model(features, adj)

    logits = np.e ** output[idx_test]
    # print(logits)

    logits = logits[:, 1]
    # print(logits)

    loss_test = F.nll_loss(output[idx_test], labels[idx_test])
    acc_test = accuracy(output[idx_test], labels[idx_test])
    print("Test set results:",
          "loss= {:.4f}".format(loss_test.item()),
          "accuracy= {:.4f}".format(acc_test.item()))
    # df_test = pd.read_csv(os.path.join(args.data_dir, "test.csv")) 
    ## ROC CURVE
    fpr, tpr, thresholds = roc_curve(labels[idx_test].cpu().detach().numpy(), logits.cpu().detach().numpy())
    auc_score = auc(fpr, tpr)
    print("ROC auc:", auc_score)
    plt.figure(1)
    plt.plot([0, 1], [0, 1], 'k--')
    plt.plot(fpr, tpr, label='ROC curve (AUC= {:.3f})'.format(auc_score))
    plt.xlabel('False positive rate')
    plt.ylabel('True positive rate')
    plt.title('ROC curve')
    plt.legend(loc='best')
    string = str(fold)+'auroc_gcn_'+args.readmission_mode+'_'+args.feature_mode+'.png'
    fig1 = plt.gcf()
    plt.show()
    plt.draw()
    fig1.savefig(os.path.join(args.output_dir, string))

    ## PR CURVE
    precision, recall, thres = precision_recall_curve(labels[idx_test].cpu().detach().numpy(), logits.cpu().detach().numpy())
    pr_thres = pd.DataFrame(data =  list(zip(precision, recall, thres)), columns = ['prec','recall','thres'])
    vote_df = pd.DataFrame(data =  list(zip(logits.cpu().detach().numpy(), labels[idx_test].cpu().detach().numpy())), columns = ['score','label'])

    temp = pr_thres[pr_thres.prec > 0.799999].reset_index()
    rp80 = 0
    if temp.size == 0:
        print('Test Sample too small or RP80=0')
    else:
        rp80 = temp.iloc[0].recall
        print('Recall at Precision of 80 = {}'.format(rp80))

    area = auc(recall, precision)
    print("PR auc:", area)
    step_kwargs = ({'step': 'post'}
                   if 'step' in signature(plt.fill_between).parameters
                   else {}) 
    plt.figure(2)
    plt.step(recall, precision, color='b', alpha=0.2,
             where='post')
    plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.ylim([0.0, 1.05])
    plt.xlim([0.0, 1.0])
    plt.title('Precision-Recall curve: AUC= {0:0.2f}'.format(
              area))   
    string = str(fold)+'auprc_gcn_'+args.readmission_mode+'_'+args.feature_mode+'.png'
    fig2 = plt.gcf()
    plt.show()
    plt.draw()
    fig2.savefig(os.path.join(args.output_dir, string))

total_num_ones = len(one_list)
total_num_zeros = len(zero_list)
cv_fold = 5
half_test = int(min(total_num_ones, total_num_zeros)/cv_fold)

for fold in range(cv_fold):
  test_index_ones = one_list[fold*half_test:(fold+1)*half_test]
  val_index_ones =  one_list[fold*half_test:(fold+1)*half_test]
  train_index_ones = one_list[0: fold*half_test] + one_list[(fold+1)*half_test:]

  test_index_zeros = zero_list[fold*half_test:(fold+1)*half_test]
  val_index_zeros =  zero_list[fold*half_test:(fold+1)*half_test]
  train_index_zeros = zero_list[0: fold*half_test] + zero_list[(fold+1)*half_test:]

  train_index_1 = [i[0] for i in train_index_ones]
  train_index_0 = [i[0] for i in train_index_zeros]
  idx_train = train_index_1 + train_index_0

  val_index_1 = [i[0] for i in val_index_ones]
  val_index_0 = [i[0] for i in val_index_zeros]
  idx_val = val_index_1 + val_index_0

  test_index_1 = [i[0] for i in test_index_ones]
  test_index_0 = [i[0] for i in test_index_zeros]
  idx_test = test_index_1 + test_index_0


  #print(torch.sum(labels[idx_train])
  train_ad_ids = [ad_ids[i] for i in idx_train]
  val_ad_ids = [ad_ids[i] for i in idx_val]
  test_ad_ids = [ad_ids[i] for i in idx_test]

  def common_data(list1, list2): 
      result = False
    
      # traverse in the 1st list 
      for x in list1: 
    
          # traverse in the 2nd list 
          for y in list2: 
      
              # if one common 
              if x == y: 
                  result = True
                  return result  
                    
      return result 
        
  print("common data between train and val:", common_data(train_ad_ids, val_ad_ids)) 
  print("common data between train and test:", common_data(test_ad_ids, train_ad_ids)) 

  idx_train = torch.LongTensor(idx_train)
  idx_val = torch.LongTensor(idx_val)
  idx_test = torch.LongTensor(idx_test)

  model = GCN(nfeat=features.shape[1],
            nhid=args.hidden,
            nclass=labels.max().item() + 1,
            dropout=args.dropout)
  
  optimizer = optim.Adam(model.parameters(),
                        lr=args.lr, weight_decay=args.weight_decay)

  args.cuda = not args.no_cuda and torch.cuda.is_available()
  if args.cuda:
      model.cuda()
      features = features.cuda()
      adj = adj.cuda()
      labels = labels.cuda()
      idx_train = idx_train.cuda()
      idx_val = idx_val.cuda()
      idx_test = idx_test.cuda()

  # Train model
  t_total = time.time()
  print("num of fold:", fold)
  for epoch in range(args.epochs):
      train(epoch)
  print("Optimization Finished!")
  print("Total time elapsed: {:.4f}s".format(time.time() - t_total))

  # Testing
  test(fold)

## save features for future use
# if args.save_features:
#   features_df = pd.DataFrame({'ids':ad_ids, 'features':all_mean_embeddings, 'labels':labels},
#                            columns= ['ids', 'features', 'labels'])
#   features_df.to_pickle(args.data_dir+'dis_pooled_features')

# Model and optimizer
